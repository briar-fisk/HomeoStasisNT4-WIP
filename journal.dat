
Wed 10/04/2023  1:55:46.96:
   Beginning constructino on the new NT4 network. Stripped down, only std and other cross platform stuff. Won't be as pretty though as the console color stuff is windows specific.

Wed 10/04/2023  1:56:53.18:
   Setup a new project. Added the NT4.h file, starting on the c_Node.h atm

Wed 10/04/2023  3:11:57.28:
   Nodes can now accept dendritic and axonic connections. 

Wed 10/04/2023  3:16:13.99:
   Does_Lower_Connection_Exist added, checks out.

Wed 10/04/2023  3:25:51.29:
   Does_Upper_Tier_Connection_Exist added, tested, checks out.

Wed 10/04/2023  5:06:34.21:
   Added backpropagation, though will need to build the network and CAN before I can fully verify it works correctly.

Wed 10/04/2023  5:10:27.05:
   State binding added and working.

Wed 10/04/2023  5:15:17.32:
   Ok. build a simple 3x network and tested bp, seems to be working. Will verify with more complex networks when CAN and NNet are built

Fri 10/06/2023  1:01:19.97:
   Defined the basic Node_Network class, now to fill it out.

Fri 10/06/2023  1:22:29.31:
   Setup the fractal trees. One for nodes, the nodes are stored in a linked list with a fractal tree on top. A new node is created then added to the Nodes fractal tree. There are multiple state trees, a dynamic array, so that each construct can have its own input set. The nodes stored in the state tree are only references, nodes are only created when adding to the Nodes tree.

Fri 10/06/2023  3:33:41.74:
   Node network can now create nodes and create + bind state nodes.

Fri 10/06/2023  3:34:22.12:
   Took a few minutes, I am retarder and didn't create the fractal tree on index 0, kept trying to access the non-existent tree.

Fri 10/06/2023  5:10:19.59:
   Create_Connections is different than the old one, binds a bunch of nodes to p_To as dendrites, and binds all of them to p_To as axons.

Fri 10/06/2023  5:10:37.39:
   Need to make the distiction as well between normal ones and _F

Fri 10/06/2023 21:21:29.47:
   Forgot I had it ignore the first index in does_Lower_Connection_Exist so I've been chasing that down.

Fri 10/06/2023 21:25:48.89:
   Get_Upper_Tier_Node works, so does does_Upper_Tier_Connection_Exist at the c_Node_Network level.

Fri 10/06/2023 22:30:25.75:
   Get_State_Node is done. These all assume a valid construct index. Node operations should mostly be handled by the CAN and other internal systems so unless the user gets really hands on (aka gigachad) this won't be an issue.

Fri 10/06/2023 23:45:07.17:
   Starting on the Current Active Node Scaffold for Trace Encoding (CAN)

Sun 10/08/2023 22:20:54.82:
   Instead of using templates to allow interfaces of different types we will use interface classes for each datatype. This is to keep the code simpler and more accessible, so we can handle each datatype appropriately (though you can with templates but whatever), and because the states once inside the network won't be altered so we can typecase back and forth beneach the interface. This requires that any trace selection and predictuliction is done outside of the interface. If you are collapsing states that are of a string vs a float you would handle those differently than uint64_t most likely.

Sun 10/08/2023 22:26:07.66:
   Each CAN will only hold one inpute set. Each CAN is effectively a different contruct within the network.

Sun 10/08/2023 23:14:03.65:
   CAN now accepts input, making the c_CAN_Many_To_One first

Sun 10/08/2023 23:14:09.81:
   It also outputs

Sun 10/08/2023 23:14:21.69:
   Outputs the input array that it.

Sun 10/08/2023 23:14:28.62:
   *is

Sun 10/08/2023 23:57:42.60:
   reset_Scaffold and setup_CAN_Scaffold added for c_CAN_Many_To_Many

Mon 10/09/2023  0:37:27.72:
   fill_State() in c_CAN_Many_To_Many is working.

Mon 10/09/2023  0:48:22.38:
   added encode(uint64_t * p_Input, int p_Depth) which handles the set_Input, setup_CAN_Scaffold, fill_State, fill_Scaffold, and for now output_Input and output_Scaffold.

Mon 10/09/2023 23:26:03.00:
   Moved the NT4 library to a subdir in includes. 

Mon 10/09/2023 23:29:00.01:
   Added c_Granulator, c_Sim (for testing), and going to move the skeleton of the homeostasis module from the old project to the new one to begin rebuilding it upon the new network. This way I can iteratively build both the network functionality and the module at the same time. The visualizer will be where all trace selection happens, keeping it containted to there will allow for swapping out different methods for testing and development without needing to change the main program.

Tue 10/10/2023 14:24:21.34:
   Alrighty, slept on it and decided to make a polymorphic base class for the CAN structures. This way I can set it up so that you register an assembly within the NNet and this assembly is actually a CAN functioning as an interface. The granulator, visualizer, actuator interface, are going to be a separate module that sites outside the main NNet engine. They will form a 'systems control' interface for setups like the homeostasis module.

Tue 10/10/2023 14:25:52.00:
   May make higher networks like Chrono and MSC be internal to the NNet, you register and configure them, then call an update function which handles chrono shifting, treetop gathering, etc.

Tue 10/10/2023 23:20:32.64:
   The problem with open source work is that family doesn't understand and sees ((no_pay) == (not_real_work)) { freetime to bother with random bullshit; }

Tue 10/10/2023 23:22:05.71:
   Anyhow, making the CAN into a polymorphic base class so we can stick them into a single array and reference them by index to keep things congruent throughout the neural network structure. One important aspect is that the actual scaffold of pointers will be declared in the derived classes and handled internally, this is because of the wildly different scaffold structures that may be used.

Wed 10/11/2023  0:37:49.52:
   The biggest question is how to handle the scaffolds as they may vary wildly. If kept internal to the CAN then we have to do the charging and whatnot through the CAN. If we can allow access to the scaffold then a higher class will be able to access and charge, see, manipulate the trace.

Wed 10/11/2023  1:50:16.65:
   Making the get_Treetop() into get_Treetop(int p_Index = 0) so that when a CAN structure has more than one treetop (such as in cylindrical stiched base networks where output_len == input_Len) we can specify which one to get, default is 0 as most configurations will only have a single treetop.

Fri 10/13/2023 22:28:22.62:
   Query implemented, using NULL_CAN functionality

Sat 10/14/2023  1:05:41.12:
   CAN Scaffold output as Char working, super basic.

Tue 10/17/2023 13:58:10.11:
   Now that the neural network can encode symbols I'm going to build the homeostasis module to the point of prediction, then switch back to the NNet, then keep bringing both of those ahead in conjunction. Going to encapsulate the Granulation, Target Values, Deltatizer, in the c_IO class. This will allow use to give the user full control over each input/output individually. Once set up by the user the internals of the homeostasis module can then request delta values, goal values, granulated data, etc from each input accordingly.

Fri 10/20/2023  2:35:18.32:
   Separating c_Input_Goal, c_Input_Actuator, c_Output_Actuator to each be their own.

Fri 10/20/2023  2:35:44.21:
   The input uses double for now, may change to templates in the future.

Fri 10/20/2023 17:01:27.83:
   The old granulator class is getting an overhaul.

Fri 10/20/2023 17:02:28.66:
   It will now be setup so that you give it the smallest ranges first and it iteratively searches from small to large, largest being the biggest deviation from the smallest. The first index [0] is always the goal, if within this range the granulator returns 0.

Fri 10/20/2023 17:17:58.18:
   Granulator is functioning as intended. Moving on to the c_Input_Goal class for the homeostasis interface.

Fri 10/20/2023 23:18:38.73:
   Added calculate_Delta to the c_Input_Goal. 

Fri 10/20/2023 23:19:35.89:
   And the Granulator. Forgot to mention I added set_Depth(int p_Depth) and the granulator before the delta caculator.

Fri 10/20/2023 23:23:47.64:
   Added shift_Data and wipe_Data to the c_Input_Goal

Sat 10/21/2023  1:41:48.46:
   c_Input_Goal is working.

Sun 10/22/2023  1:11:55.72:
   Alrighty, instead of 'goal' and 'feedback' we now use 'afferent' and 'efferent' for biomimicry in the Gaia module branding and design.

Sun 10/22/2023  1:24:35.21:
   Hehehe, doing the registration for the IO sets I accidentally deleted the tmp var and then set the main array to NULL instead of the tmp one.

Sun 10/22/2023 20:54:40.17:
   Have the output_AE, registration for both A and E, the linking of the manipulations to the surface of the module. Testing setting, shifting, etc of the data.

Sun 10/22/2023 22:23:47.11:
   Alright, so far seems good. Now to hook the NNet up to it.

Sun 10/22/2023 22:27:45.38:
   Moved the IO to the c_IO_Interface.h

Sun 10/22/2023 22:30:26.83:
   Renamed c_IO_Interface to c_AE_Interface

Thu 10/26/2023  1:34:54.23:
   output_Gathered() added. Gets the current input set as 2 arrays, Afferent is 2d, efferent 1d

Thu 10/26/2023 22:56:21.92:
   Added the Many to one CANs in c_Homeostasis_Module, Raw_Concrete (can be left out), Raw_Granulated, Raw_Delta, and Raw_Efferent.

Fri 10/27/2023 22:48:00.56:
   For the Gaia module I need to finish hooking it up to the NNets, encoding, charging, gathering traces, sorting traces, trace synthesis and testing, and output to the efferent. In trace synthesis and testing we will be making use of the prediction capabilities to test each trace for error reduction.

Fri 10/27/2023 23:39:01.30:
   Alright alright, got the gathered inputs setup finally, forgot to resize_Gathered_Input() and had a buffer overrun, dumb error but it is fixed now.

Fri 10/27/2023 23:40:26.12:
   NNets now encode! w00t w00t! Time to get the MSC, then the Chrono hooked up

Fri 10/27/2023 23:41:47.76:
   Nvm, I was already gathering the raw treetops, so MSC is encoding as well atm.

Fri 10/27/2023 23:54:16.69:
   Chrono encoding works now.

Sun 10/29/2023 17:16:14.39:
   Did a redesign on the c_Node so that axons are stored in one array sub-divided by the index of the denrite they connect too. Where before _F mean the first, now it is index [0] representing 'axon hillock' 0. This is so that during charging we can specify which legs to fire upon which is essential for the temporal searching in chronological networks where we use left and right leg charging to search forward and backwards in the time series encodings.

Sun 10/29/2023 18:32:38.44:
   So here's what I'm thinking, we move the leg charging flags to a referenced object from the node to the CAN structure. This allows us to register and control leg behavior through the CAN structure. Then we can allow the charging buffers to run after requesting the CAN structures update and collect the treetops from the end. If we backpropagated out the trace then we could output the patterns of quanta as they cascade out. There would be many of the same trace but it would just add more detail and it looks cooler.

Sun 10/29/2023 18:35:56.30:
   The only problem with this is that it implements a bias that is mutually exclusive in that only one node can be linked to one CAN meaning it excludes and other structures from sharing the nodes, like a spacial dimension. You can still build structures on top of the same input set but you need to separate them if you want different charging schemas.

Sun 10/29/2023 18:38:21.56:
   I just realized I need to change the MT1 so that it loads in the lower construct nodes on the state node tier. Otherwise the data has a rupture between lower and higher constructs and the charging buffers will break on it when they roll that high.

Sun 10/29/2023 18:46:52.80:
   set_Leg_Count and set_Leg_Firing_Order added to the base class for CAN structures.

Mon 10/30/2023 18:09:14.89:
   Hmm, so for the problem of making multi-sensory constructs but allowing different CAN structures we have to address the input tier problem, mainly that right now each CAN has an input set that only accepts states. I'm thinking if we add another array dictating 'type' of input then we could easily integrate node replacement instead of state linking allowing us to read in treetops directly. An extra array that says if it is a state input or node input.

Mon 10/30/2023 18:19:12.87:
   So after some paper exploration I found that the backpropagation algorithm breaks down with a state tier connecting directly to treetops below. The solution I believe is to attach types to the nodes so we know which is state, memory, and treetop. Possibly use parallel trees in the Node_Network, or give each node a value to hold detailing the node type. Adding the trees is the likely route as each I think assigning a value to each node may be the way to go. The biggest benefit of the tree is that it would be nice to have an easy handle on each node type if you ever wanted to look them up.

Mon 10/30/2023 19:29:56.79:
   I am reusing the old charging buffer. Removing charge_spike for now. 

Mon 10/30/2023 19:32:18.71:
   Instead of Left/Right inside of the charging buffer we now iterate through each axon hillock. I think I should call them processess but idk, been awhile since I've looked at the biological morphologies they were modeled on.

Mon 10/30/2023 19:34:40.69:
   Thinking of wrapping the Charging_Buffer and a CAN structure on the same level in the construct class to flatten what the user sees.

Mon 10/30/2023 19:36:21.60:
   Removed RC charging from the charging buffer. It will be added back but likely very differently.

Mon 10/30/2023 20:04:02.76:
   Altered the types, 0 is now NULL and the rest are 1-state, 2-branch, 3-treetop, 4-state-treetop

Mon 10/30/2023 20:16:43.04:
   Added output_BP to the module which gives surface access to the nodes output which I just added called output_BP which iteratively goes through the linked list of nodes and outputs the bp_O() on them with the new type checking for proper discharging to handle the issue of treetops as intputs from discrete lower tier constructs.

Mon 10/30/2023 22:25:21.11:
   Converted all internal states to double throughout, will template it later when I have the internet to look up the syntax again.

Mon 10/30/2023 22:49:21.48:
   So for now I'm going to make a union for the interface and change everything internally back to uint64_t

Tue 10/31/2023 10:39:13.69:
   If everything interally is uint64_t or some other common type I can keep the union limited to just the AE interface and use explicitly named members to get and set desired values. Avoid the mess that happened in NT3 with u_DATA_3.

Tue 10/31/2023 10:47:42.86:
   set_voidstar lol

Tue 10/31/2023 10:50:12.88:
   Adding the u_Data and a wrapper class to c_AE_Interface that will handle the unionized data (self propagating organizations are THE DEVIL) so that when the user calls set_double, set_uint64_t, etc it will set the type appropriately. Although, if I'm using explicit get and set for datatype I don't need the type var as that will be up to the user when they call get_uint64_t or whatever.

Wed 11/01/2023  5:13:01.49:
   uint64_t internally is ugly for debugging but works. I had an issue last time with the union and trying to account for all the datatypes it handled. I will likely make a base class, possibly polymorphic, for the node. 

Wed 11/01/2023  5:14:25.57:
   Stealing the charge_Buffer_C from c_NT3_Construct_1D as I didn't feel like rewriting the logic to charge it properly. To use it you do .charge_Outputs() .submit() .gather() while(!Done)

Wed 11/01/2023  5:27:52.22:
   Forgot to initialize the leg firing order to true. Charging buffers not firing correctly atm.

Wed 11/01/2023  6:05:36.46:
   I had the modifier charge set to 0 in a dumb dumb move. It wouldn't charge because the chargesd were put on their knees and executed the moment they tried to excite a node.

Wed 11/01/2023  6:17:38.24:
   w00t w00t, looks like it is charging now! Still need to hand verify it, but so far much progress.
